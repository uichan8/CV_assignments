{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 패키지 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'student_20172099'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k5/mdqvz0d17yg6tk40xj22_cq80000gn/T/ipykernel_40632/2190785785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstudent_20172099\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheat_interest_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_correspondence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'student_20172099'"
     ]
    }
   ],
   "source": [
    "# Local Feature Stencil Code\n",
    "# Written by James Hays for CS 143 @ Brown / CS 4476/6476 @ Georgia Tech with Henry Hu <henryhu@gatech.edu>\n",
    "# Edited by James Tompkin\n",
    "# Adapted for python by asabel and jdemari1 (2019)\n",
    "# Modified by Soochahn Lee for Computer Vision coarse, School of Electrical Engineering, Kookmin University\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, filters, feature, img_as_float32\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import student_20172099 as student\n",
    "import visualize\n",
    "from helpers import cheat_interest_points, evaluate_correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 실험용 데이터 파일 로딩 함수 정의\n",
    "\n",
    "해당 함수는 실험용 영상의 쌍을 읽어들이고 적절하게 포맷을 조정하는 코드와, 해당 영상 쌍에 대한 특징점 정합의 참값 데이터를 읽어들이는 코드로 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads preassigned images and data for test cases\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "     1) Load stuff\n",
    "     There are numerous other image sets in the supplementary data on the\n",
    "     project web page. You can simply download images off the Internet, as\n",
    "     well. However, the evaluation function at the bottom of this script will\n",
    "     only work for three particular image pairs (unless you add ground truth\n",
    "     annotations for other image pairs). It is suggested that you only work\n",
    "     with the two Notre Dame images until you are satisfied with your\n",
    "     implementation and ready to test on additional images. A single scale\n",
    "     pipeline works fine for these two images (and will give you full credit\n",
    "     for this project), but you will need local features at multiple scales to\n",
    "     handle harder cases.\n",
    "\n",
    "     If you want to add new images to test, create a new elif of the same format as those\n",
    "     for notre_dame, mt_rushmore, etc. You do not need to set the eval_file variable unless\n",
    "     you hand create a ground truth annotations. To run with your new images use\n",
    "     python main.py -p <your file name>.\n",
    "\n",
    "    :param file_name: string for which image pair to compute correspondence for\n",
    "\n",
    "        The first three strings can be used as shortcuts to the\n",
    "        data files we give you\n",
    "\n",
    "        1. notre_dame\n",
    "        2. mt_rushmore\n",
    "        3. e_gaudi\n",
    "\n",
    "    :return: a tuple of the format (image1, image2, eval file)\n",
    "    \"\"\"\n",
    "    # Note: these files default to notre dame, unless otherwise specified\n",
    "    image1_file = \"../data/NotreDame/NotreDame1.jpg\"\n",
    "    image2_file = \"../data/NotreDame/NotreDame2.jpg\"\n",
    "\n",
    "    eval_file = \"../data/NotreDame/NotreDameEval.mat\"\n",
    "\n",
    "    if file_name == \"notre_dame\":\n",
    "        pass\n",
    "    elif file_name == \"mt_rushmore\":\n",
    "        image1_file = \"../data/MountRushmore/Mount_Rushmore1.jpg\"\n",
    "        image2_file = \"../data/MountRushmore/Mount_Rushmore2.jpg\"\n",
    "        eval_file = \"../data/MountRushmore/MountRushmoreEval.mat\"\n",
    "    elif file_name == \"e_gaudi\":\n",
    "        image1_file = \"../data/EpiscopalGaudi/EGaudi_1.jpg\"\n",
    "        image2_file = \"../data/EpiscopalGaudi/EGaudi_2.jpg\"\n",
    "        eval_file = \"../data/EpiscopalGaudi/EGaudiEval.mat\"\n",
    "\n",
    "    image1 = img_as_float32(io.imread(image1_file))\n",
    "    image2 = img_as_float32(io.imread(image2_file))\n",
    "\n",
    "    return image1, image2, eval_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 실험용 영상 설정 \n",
    "\n",
    "본 notebook에서 아래 셀은 학생 여러분이 직접 아래 세 가지 중 입력을 하도록 변경해야 함\n",
    "   1. notre_dame\n",
    "   2. mt_rushmore\n",
    "   3. e_gaudi\n",
    "\n",
    "반드시 영상 쌍을 세 종류 모두에 대해 실험하고 결과를 도출하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) SET TEST PAIR NAME\n",
    "#   EITHER \"notre_dame\", \"mt_rushmore\", or \"e_gaudi\"\n",
    "pair_name = \"notre_dame\"  # <-- YOU CAN CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 영상 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Load and resizes images\n",
    "image1_color, image2_color, eval_file = load_data(pair_name)\n",
    "\n",
    "# Let's work with grayscale images.\n",
    "image1 = rgb2gray(image1_color)\n",
    "image2 = rgb2gray(image2_color)\n",
    "\n",
    "# make images smaller to speed up the algorithm. This parameter\n",
    "# gets passed into the evaluation code, so don't resize the images\n",
    "# except for changing this parameter - We will evaluate your code using\n",
    "# scale_factor = 0.5, so be aware of this\n",
    "scale_factor = 0.5\n",
    "\n",
    "# Bilinear rescaling\n",
    "image1 = np.float32(rescale(image1, scale_factor))\n",
    "image2 = np.float32(rescale(image2, scale_factor))\n",
    "\n",
    "# width and height of each local feature, in pixels\n",
    "feature_width = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 특징점 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (you code this)\n",
    "# (2) Find distinctive points in each image. See Szeliski 4.1.1\n",
    "# !!! You will need to implement get_interest_points. !!!\n",
    "\n",
    "print(\"Getting interest points...\")\n",
    "\n",
    "(x1, y1) = student.get_interest_points(image1,feature_width)\n",
    "(x2, y2) = student.get_interest_points(image2,feature_width)\n",
    "\n",
    "# For development and debugging you can compare with the ta ground truth points\n",
    "# by uncommenting the following lines.\n",
    "# Note that the ground truth points for mt. rushmore will not produce good results.\n",
    "\n",
    "# (x1, y1, x2, y2) = cheat_interest_points(eval_file, scale_factor)\n",
    "\n",
    "# view your corners!\n",
    "\n",
    "plt.imshow(image1, cmap=\"gray\")\n",
    "plt.scatter(x1, y1, alpha=0.9, s=3)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(image2, cmap=\"gray\")\n",
    "plt.scatter(x2, y2, alpha=0.9, s=3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 특징 기술자 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (you code this)\n",
    "# 3) Create feature vectors at each interest point. Szeliski 4.1.2\n",
    "# !!! You will need to implement get_features. !!!\n",
    "\n",
    "print(\"Getting features...\")\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "image1_features = student.get_features(image1, x1, y1, feature_width)\n",
    "image2_features = student.get_features(image2, x2, y2, feature_width)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 특징점 정합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (you code this)\n",
    "# 4) Match features. Szeliski 4.1.3\n",
    "# !!! You will need to implement match_features !!!\n",
    "\n",
    "print(\"Matching features...\")\n",
    "\n",
    "matches, confidences = student.match_features(image1_features, image2_features)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 참값 데이터를 기반으로 도출된 결과의 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Evaluation and visualization\n",
    "\n",
    "# The last thing to do is to check how your code performs on the image pairs\n",
    "# we've provided. The evaluate_correspondence function below will print out\n",
    "# the accuracy of your feature matching for your 50 most confident matches,\n",
    "# 100 most confident matches, and all your matches. It will then visualize\n",
    "# the matches by drawing green lines between points for correct matches and\n",
    "# red lines for incorrect matches. The visualizer will show the top\n",
    "# num_pts_to_visualize most confident matches, so feel free to change the\n",
    "# parameter to whatever you like.\n",
    "\n",
    "print(\"Matches: \" + str(matches.shape[0]))\n",
    "\n",
    "num_pts_to_visualize = 50\n",
    "\n",
    "evaluate_correspondence(image1_color, image2_color, eval_file, scale_factor,\n",
    "    x1, y1, x2, y2, matches, confidences, num_pts_to_visualize, pair_name + '_matches.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
